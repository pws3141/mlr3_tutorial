---
title: "Applied Machine Learning Using mlr3"
subtitle: "Evaluation and Benchmarking"
author: "Paul Smith"
date: today
format:
  html:
    embed-resources: true
    code-annotations: hover
    grid:
      margin-width: 350px
    fig-width: 6
    fig-asp: 0.618
    toc: true
    toc-location: left
bibliography: ref.bib
reference-location: margin
citation-location: margin
fig-cap-location: margin
tbl-cap-location: margin
lst-cap-location: margin
link-citations: true
---

# Introduction and prerequisites

The `{mlr3}` package uses the R6 class system alongside `data.table` to store and operate on tabular data.

For a brief introduction to the R6 class system, see [Section
1.5.1](https://mlr3book.mlr-org.com/chapters/chapter1/introduction_and_overview.html#sec-r6)
of the mlr3 book, or the package
[vignette](https://r6.r-lib.org/articles/Introduction.html). 

For information on the `data.table` format, see the package [vignette](https://cran.r-project.org/web/packages/data.table/vignettes/datatable-intro.html).

This article follows the tutorial given in the *Applied Machine Learning Using
mlr3 in R*
[online notes](https://mlr3book.mlr-org.com/chapters/chapter3/evaluation_and_benchmarking.html) [@casalicchio2024evaluation].

```{r}
library(mlr3)
library(mlr3viz)
library(mlr3learners)
library(ggplot2)
library(patchwork)
library(data.table)
options(datatable.print.nrows = 20)
library(palmerpenguins)
```


## Structure

This article is structured as follows:

- Model building and validation using the data-splitting technique is discussed in @sec-datasplitting.
- Resampling techniques are discussed in @sec-resampling.
- @sec-benchmarking shows ways to compared models.
- @sec-binary-classification gives specific performance measures for binary classification.

A reference for more in-depth detail of measures and performance estimation is @japkowicz2011evaluating.

# Data-splitting and Scoring {#sec-datasplitting}

This is a bad method (see my posts --
[one](https://pws3141.github.io/blog/posts/06-stepwise_datasplitting/) and
[two](https://pws3141.github.io/blog/posts/07-stepwise_datasplitting_simulation/)
-- for more information). Anyway, let's do it.

```{r}
# the task is already loading in {mlr3}
#tsk_penguins = tsk("penguins")
# but, let's do it from scratch instead
data("penguins", package = "palmerpenguins")
set.seed(1)

penguins_dt <- as.data.table(penguins)
tsk_penguins <- as_task_classif(penguins_dt, target = "species", id = "penguins")
tsk_penguins 

splits = partition(tsk_penguins, ratio = 0.67)
lrn_rpart = lrn("classif.rpart")
lrn_rpart$train(tsk_penguins, splits$train)
prediction = lrn_rpart$predict(tsk_penguins, splits$test)
prediction$score(msr("classif.acc"))
```

:::{.column-margin}
![](./fig/penguin_score.svg)
:::

# Resampling Methods {#sec-resampling}

![A general abstraction of the performance estimation process. The available data is (repeatedly) split into training data and test data (data splitting / resampling process). The learner is trained on each training dataset and produces intermediate models (learning process). Each intermediate model makes predictions based on the features in the test data. The performance measure compares these predictions with the ground truth from the test data and computes a performance value for each test dataset. All performance values are aggregated into a scalar value to estimate the generalization performance (evaluation process).](./fig/resampling.svg)

This section focusses on Cross-Validation. In the tutorial, the authors state that:

> Bootstrapping has become less common, as having repeated observations in
training data can lead to problems in some machine learning setups, especially
when combined with model selection methods and nested resampling (as duplicated
observations can then end up simultaneously in training and test sets in nested
schemes).

I'm not sure this is true, and I don't think Frank Harrell would agree with
this statement, for example in his comment on the
[datamethods](https://discourse.datamethods.org/t/bootstrap-vs-cross-validation-for-model-performance/2779)
website.

:::{.column-margin}
For an explanation of using bootstrap to assess internal validation of a model, see the [stackexchange](https://stats.stackexchange.com/questions/466851/combining-bootstrap-and-cross-validation) post. For more detail, refer to @seyerberg2019statistical.
:::

![Illustration of three-fold cross-validation](./fig/cross_validation.svg)

## Constructing a resampling strategy

Resampling strategies are stored in the `mlr_resamplings` dictionary.

```{r}
as.data.table(mlr_resamplings)
```

The `rsmp()` function is used to construct a `Resampling` object. For example, to create a holdout strategy with a 4/5 split,

```{r}
rsmp("holdout", ratio = 0.8)
```

Or other strategies:

```{r}
# three-fold CV
cv3 = rsmp("cv", folds = 3)
# Subsampling with 3 repeats and 9/10 ratio
ss390 = rsmp("subsampling", repeats = 3, ratio = 0.9)
# 2-repeats 5-fold CV
rcv25 = rsmp("repeated_cv", repeats = 2, folds = 5)
```

The `Resampling` object created gives a definition for how the data splitting process will be performed on a task. It is possible to manually instantiate a resampling strategy using `$instantiate()`:

```{r}
cv3$instantiate(tsk_penguins)
# first 10 observations in the second training set
cv3$train_set(2)[1:10]
# first 10 observations in the third test set 
cv3$test_set(3)[1:10]
```

Note that `$instantiate()` is rarely required, as we can use the `resample()`
method to do it all.

## Resampling experiments

The `resample()` function requires a `Task`, `Learner` and `Resampling` objects. It stores the result in an `ResampleResult` object.

```{r}
rr <- resample(tsk_penguins, lrn_rpart, cv3)
rr
```

We can calculate the score for each iteration using `$score()`.^[By default,
`score()` evaluates the performance in the test sets in each iteration.]

```{r}
acc <- rr$score(msr("classif.ce"))
acc
```

The score can also be aggregated over the iterations. By default, the
`classif.ce` score uses *macro* average, where the score is calculated for
each iteration, and then averaged. The *micro* average can be computed, where
the predictions are pooled across iterations into one `Prediction` object, and
then a single score calculated.

```{r}
# the default aggregation method can be found in the $average field
msr("classif.ce")$average
# macro
rr$aggregate(msr("classif.ce"))
# micro
rr$aggregate(msr("classif.ce", average = "micro"))
```


![An example of the difference between `$score()` and `$aggregate()`](./fig/aggregation.svg)

The results can be visualised using the `autoplot.ResampleResult()` function.

```{r}
#| fig-cap: Boxplot (left) and histogram (right) of accuracy scores for 10-fold CV
#| warning: false

rr = resample(tsk_penguins, lrn_rpart, rsmp("cv", folds = 10))
gg1 <- autoplot(rr, measure = msr("classif.acc"), type = "boxplot")
gg2 <- autoplot(rr, measure = msr("classif.acc"), type = "histogram")
gg1 + gg2
```

## ResampleResult objects

In general, the intermediate models produced for each iterations are not stored
in the `ResampleResult` object, to save memory. However, we can configure the
`resample()` function to keep these models.

```{r}
#| output: false
rr <- resample(tsk_penguins, lrn_rpart, cv3, store_models = TRUE)
```

:::{.panel-tabset}

## Model 1
```{r}
rr$learners[[1]]
rr$learners[[1]]$model
```
## Model 2
```{r}
rr$learners[[2]]
rr$learners[[2]]$model
```
## Model 3
```{r}
rr$learners[[3]]
rr$learners[[3]]$model
```
:::

We can also use this to inspect the most important variables in each iteration to help us learn more about the models.

```{r}
# print 2nd and 3rd iteration
lapply(rr$learners, function(x) x$model$variable.importance)
```

## Custom resampling

If custom resampling is required, then the `rsmp("custom")` function can be used. See [Section 3.2.4](https://mlr3book.mlr-org.com/chapters/chapter3/evaluation_and_benchmarking.html#sec-resamp-custom) of the {mlr3} tutorial to learn more.

## Stratification and grouping

It is possible to group or stratify observations. Grouped sampling can be used to ensure that all observations of the same group (*e.g.* country) belong to either the training or test set, to prevent leaking information. Stratified sampling ensures that one or more discrete features within the training of test sets will have a similar distribution an in the original task containing all observations.

For information, see [Section 3.2.5](https://mlr3book.mlr-org.com/chapters/chapter3/evaluation_and_benchmarking.html#sec-strat-group) of the {mlr3} tutorial.

# Benchmarking {#sec-benchmarking}

Benchmarking
: the comparison of different learners on one or more tasks.

## `benchmark()`

The `benchmark()` function runs `resample()` on each task and learner separately and collates the results.

To use `benchmark()` we first need to call `benchmark_grid()`, which constructs an exhaustive design to describe all combinations of the learners, tasks and resamplings to be used in a benchmark experiment.^[By default, `benchmark_grid()` instantiates the resamplings on the tasks, which
means that concrete train-test splits are generated. Since this process is
stochastic, it is necessary to set a seed before calling `benchmark_grid()` to
ensure reproducibility of the data splits.]

```{r}
tasks = tsks(c("german_credit", "sonar"))
# 'classif.ranger' is a random classification forest
learners = lrns(c("classif.rpart", "classif.ranger",
  "classif.featureless"), predict_type = "prob")
rsmp_cv5 = rsmp("cv", folds = 5)

design = benchmark_grid(tasks, learners, rsmp_cv5)
design
```

Then we can run an experiement on the benchmark design.

```{r}
#| echo: false
#| output: false

bmr = benchmark(design)
```
```r
bmr = benchmark(design)
bmr
```
```{r}
#| echo: false
bmr
```

As `benchmark()` is an extension of `resample()`, we can obtain the score, and
aggregate in the same way.

```{r}
bmr$score()[, .(iteration, task_id, learner_id, classif.ce)]
bmr$aggregate()[, .(task_id, learner_id, classif.ce)]
```

Here, we can possibly conclude that the random forest is the best of all three
models on each task, although some analysis will have to be done to assert
this confidently.^[How to do this analysis comes much later in the {mlr3} [tutorial](https://mlr3book.mlr-org.com/chapters/chapter11/large-scale_benchmarking.html#sec-benchmark-analysis)]

### BenchmarkResult objects
A `BenchmarkResult` object is a collection of multiple `ResampleResult` objects.
It stores and organizes the results of benchmarking multiple machine learning
models (`Learners`) on one or more tasks. It is created when you run a benchmark
experiment using `benchmark()`.

```{r}
bmrdt = as.data.table(bmr)
bmrdt[, .(task, learner, resampling, iteration)]
```

The `ResampleResults` can be extracted via the `$resample_result(i)`.

```{r}
rr1 = bmr$resample_result(1)
rr1
rr2 = bmr$resample_result(2)
```

And, `as_benchmark_result()` can be used to convert objects from `ResampleResult` to `BenchmarkResult`.

```{r}
bmr1 = as_benchmark_result(rr1)
bmr2 = as_benchmark_result(rr2)

c(bmr1, bmr2)
```

```{r}
#| fig-cap: Boxplots of accuracy scores for each learner across resampling iterations and the three tasks.
autoplot(bmr, measure = msr("classif.acc"))
```

# Evaluating Binary Classifiers {#sec-binary-classification}

Here, we look at using the confusion matrix (@sec-binary-confusion), and ROC analysis (@sec-binary-roc) to evaluate binary classifiers.

## Confusion matrix {#sec-binary-confusion}

Confusion matrices give:

- True positives (TPs): Positive instances that are correctly classified as positive.
- True negatives (TNs): Negative instances that are correctly classified as negative.
- False positives (FPs): Negative instances that are incorrectly classified as positive.
- False negatives (FNs): Positive instances that are incorrectly classified as negative.

```{r}
tsk_german = tsk("german_credit")
lrn_ranger = lrn("classif.ranger", predict_type = "prob")
splits = partition(tsk_german, ratio = 0.8)

lrn_ranger$train(tsk_german, splits$train)
prediction = lrn_ranger$predict(tsk_german, splits$test)
prediction$score(msr("classif.acc"))
```
```{r}
#| lst-label: lst-binary-confusion
#| lst-cap: The confusion matrix for the random forest learner on the 'german_credit' data.
prediction$confusion
```

Here, although the accuracy score might be OK, it is clear that the predictions
are not very good at correctly predicting the 'bad' outcomes.

Normalised measures can be used to account for class imbalances.

- True Positive Rate (TPR), Sensitivity or Recall: How many of the true positives did we predict as positive?
- True Negative Rate (TNR) or Specificity: How many of the true negatives did we predict as negative?
- False Positive Rate (FPR), or $1 -$ Specificity: How many of the true negatives did we predict as positive?
- Positive Predictive Value (PPV) or Precision: If we predict positive how likely is it a true positive?
- Negative Predictive Value (NPV): If we predict negative how likely is it a true negative?
- Accuracy (ACC): The proportion of correctly classified instances out of the total number of instances.
- F1-score: The harmonic mean of precision and recall, which balances the trade-off between precision and recall. It is calculated as

$$
2 \times \frac{\text{Precision} \times \text{Recall}}{{Precision} + \text{Recall}}
$$

![Binary confusion matrix of ground truth class vs. predicted class](./fig/confusion_matrix.svg)

These measures can be computed using the `{mlr3measures}` package.

```{r}
mlr3measures::confusion_matrix(truth = prediction$truth,
  response = prediction$response, positive = tsk_german$positive)
```

The false positive rate is quite high, as expected from looking at the
confusion matrix obtained from `prediction$confusion` in @lst-binary-confusion.
There is generally a trade off between FPR and TPR. Using `predict_type
= "prob"` to obtain predicted probabilities of being in the positive class, we
can change the threshold to cut off when we assign observations to the positive
/ negative class. Increasing this threshold for identifying positive cases will
lead to a higher number of negative predictions -- therefore a lower (better)
FPR but a lower (worse) TPR (and vice-versa).

## ROC analysis {#sec-binary-roc}

ROC (Receiver Operating Characteristic) analysis is widely used to evaluate
binary classifiers by visualizing the trade-off between the TPR and the FPR.

We can use the previous `Prediction` object to compute all possible TPR and FPR
combinations by thresholding the predicted probabilities across all possible
thresholds
This is what is done in `mlr3viz::autoplot.PredictionClassif` when `type = "roc"` is chosen.
FPR is on the $x$-axis, and TPR on the $y$-axis.

```{r}
autoplot(prediction, type = "roc")
```

The area under the curve (AUC) of a ROC can be interprested as the probability
that a randomly chosen positive instance has a higher predicted probability of
belonging to the positive class than a randomly chosen negative instance.

```{r}
prediction$score(msr("classif.auc"))
```

The precision-recall curve (PRC) can also be plotted. This visualises the precision (PPV) vs. recall (TPR). The main difference between PRC and ROC is that true-negatives are ignored in the former, which can be useful in imbalanced populations where the positive class is rare.

```{r}
autoplot(prediction, type = "prc")
```

We can also see how the performance metrics change with respect to the threshold. Here, we plot the FPR and the accuracy.

```{r}
#| fig.width: 7.5
#| fig.asp: 0.5
#| label: Comparing threshold and FPR (left) with threshold and accuracy (right) for the random forest trained on tsk("german_credit").

gg1 <- autoplot(prediction, type = "threshold", measure = msr("classif.fpr"))
gg2 <- autoplot(prediction, type = "threshold", measure = msr("classif.acc"))

gg1 + gg2
```

We can look at the variability for each resampling iteration via the `ResampleResult` object.

```{r}
#| fig.width: 7.5
#| fig.asp: 0.5

rr = resample(
  task = tsk("german_credit"),
  learner = lrn("classif.ranger", predict_type = "prob"),
  resampling = rsmp("cv", folds = 5)
)

gg1 <- autoplot(rr, type = "roc")
gg2 <- autoplot(rr, type = "prc")

gg1 + gg2
```

And, we can use the `BenchmarkResult` to compare multiple learners on the same `Task`.

```{r}
#| fig.width: 8
#| fig.asp: 0.5

design = benchmark_grid(
  tasks = tsk("german_credit"),
  learners = lrns(c("classif.rpart", "classif.ranger"),
    predict_type = "prob"),
  resamplings = rsmp("cv", folds = 5)
)
bmr = benchmark(design)

autoplot(bmr, type = "roc") + autoplot(bmr, type = "prc") +
  plot_layout(guides = "collect")
```

:::{.column-margin}
I'm not sure why in the ROC plot, the $y$-axis is labeled as 'Sensitivity', whilst in the PRC, the $x$-axis is labeled 'Recall'. Maybe it's due to historical reasons?
:::

# Conclusion

Here, we learned how to estimate the generalisation performance of a model via resampling (holdout, CV, bootstrap), and how to automate the comparison of multiple learners in benchmark experiments.

| Class               | Constructor/Function      | Fields/Methods                                                       |
|---------------------|-------------------------|----------------------------------------------------------------------|
| `PredictionClassif` | `classif_lrn$predict()`  | `confusion_matrix()`; `autoplot(some_prediction_classif, type = "roc")` |
| -                  | `partition()`            | -                                                                  |
| `Resampling`       | `rsmp()`                 | `$instantiate()`                                                     |
| `ResampleResult`   | `resample()`             | `$score()`; `$aggregate()`; `$predictions()`; `as_benchmark_result()`; `autoplot(some_resample_result, type = "roc")` |
| -                  | `benchmark_grid()`       | -                                                                  |
| `BenchmarkResult`  | `benchmark()`            | `$aggregate()`; `$resample_result()`; `$score()`; `autoplot(some_benchmark_result, type = "roc")` |

# Exercises

1. Apply a repeated cross-validation resampling strategy on `tsk("mtcars")` and evaluate the performance of `lrn("regr.rpart")`.
   - Use five repeats of three folds each.
   - Calculate the MSE for each iteration and visualize the result.
   - Finally, calculate the aggregated performance score.

2. Use `tsk("spam")` and five-fold CV to benchmark `lrn("classif.ranger")`, `lrn("classif.log_reg")`, and `lrn("classif.xgboost", nrounds = 100)` with respect to AUC.
   - Which learner appears to perform best?
   - How confident are you in your conclusion?
   - Think about the stability of results and investigate this by re-running the experiment with different seeds.
   - What can be done to improve this?

3. A colleague reports a 93.1% classification accuracy using `lrn("classif.rpart")` on `tsk("penguins_simple")`.
   - You want to reproduce their results and ask them about their resampling strategy.
   - They said they used a custom three-fold CV with folds assigned as `factor(task$row_ids %% 3)`.
   - See if you can reproduce their results.

4. (\*) Program your own ROC plotting function **without** using `mlr3`'s `autoplot()` function.
   - The signature of your function should be `my_roc_plot(task, learner, train_indices, test_indices)`.
   - Your function should use the `$set_threshold()` method of `Prediction`, as well as `mlr3measures`.
