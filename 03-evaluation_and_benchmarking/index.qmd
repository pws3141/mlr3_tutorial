---
title: "Applied Machine Learning Using mlr3"
subtitle: "Evaluation and Benchmarking"
author: "Paul Smith"
date: today
format:
  html:
    embed-resources: true
    code-annotations: hover
    grid:
      margin-width: 350px
    fig-width: 6
    fig-asp: 0.618
    toc: true
    toc-location: left
bibliography: ref.bib
reference-location: margin
citation-location: margin
fig-cap-location: margin
tbl-cap-location: margin
link-citations: true
---

# Introduction and prerequisites

The `{mlr3}` package uses the R6 class system alongside `data.table` to store and operate on tabular data.

For a brief introduction to the R6 class system, see [Section
1.5.1](https://mlr3book.mlr-org.com/chapters/chapter1/introduction_and_overview.html#sec-r6)
of the mlr3 book, or the package
[vignette](https://r6.r-lib.org/articles/Introduction.html). 

For information on the `data.table` format, see the package [vignette](https://cran.r-project.org/web/packages/data.table/vignettes/datatable-intro.html).

This article follows the tutorial given in the *Applied Machine Learning Using
mlr3 in R*
[online notes](https://mlr3book.mlr-org.com/chapters/chapter3/evaluation_and_benchmarking.html)

```{r}
library(mlr3)
library(mlr3viz)
library(mlr3learners)
library(ggplot2)
library(patchwork)
library(data.table)
options(datatable.print.nrows = 20)
library(palmerpenguins)
```


## Structure

This article is structured as follows:

- Model building and validation using the data-splitting technique is discussed in @sec-datasplitting.
- Resampling techniques are discussed in @sec-resampling.
- @sec-benchmarking shows ways to compared models.
- @sec-binary-classification gives specific performance measures for binary classification.

A reference for more in-depth detail of measures and performance estimation is @japkowicz2011evaluating.

# Data-splitting and Scoring {#sec-datasplitting}

This is a bad method (see my posts --
[one](https://pws3141.github.io/blog/posts/06-stepwise_datasplitting/) and
[two](https://pws3141.github.io/blog/posts/07-stepwise_datasplitting_simulation/)
-- for more information). Anyway, let's do it.

```{r}
# the task is already loading in {mlr3}
#tsk_penguins = tsk("penguins")
# but, let's do it from scratch instead
data("penguins", package = "palmerpenguins")
set.seed(1)

penguins_dt <- as.data.table(penguins)
tsk_penguins <- as_task_classif(penguins_dt, target = "species", id = "penguins")
tsk_penguins 

splits = partition(tsk_penguins, ratio = 0.67)
lrn_rpart = lrn("classif.rpart")
lrn_rpart$train(tsk_penguins, splits$train)
prediction = lrn_rpart$predict(tsk_penguins, splits$test)
prediction$score(msr("classif.acc"))
```

:::{.column-margin}
![](./fig/penguin_score.svg)
:::

# Resampling Methods {#sec-resampling}

![A general abstraction of the performance estimation process. The available data is (repeatedly) split into training data and test data (data splitting / resampling process). The learner is trained on each training dataset and produces intermediate models (learning process). Each intermediate model makes predictions based on the features in the test data. The performance measure compares these predictions with the ground truth from the test data and computes a performance value for each test dataset. All performance values are aggregated into a scalar value to estimate the generalization performance (evaluation process).](./fig/resampling.svg)

This section focusses on Cross-Validation. In the tutorial, the authors state that:

> Bootstrapping has become less common, as having repeated observations in
training data can lead to problems in some machine learning setups, especially
when combined with model selection methods and nested resampling (as duplicated
observations can then end up simultaneously in training and test sets in nested
schemes).

I'm not sure this is true, and I don't think Frank Harrell would agree with
this statement, for example in his comment on the
[datamethods](https://discourse.datamethods.org/t/bootstrap-vs-cross-validation-for-model-performance/2779)
website.

:::{.column-margin}
For an explanation of using bootstrap to assess internal validation of a model, see the [stackexchange](https://stats.stackexchange.com/questions/466851/combining-bootstrap-and-cross-validation) post. For more detail, refer to @seyerberg2019statistical.
:::

![Illustration of three-fold cross-validation](./fig/cross_validation.svg)

## Constructing a resampling strategy

Resampling strategies are stored in the `mlr_resamplings` dictionary.

```{r}
as.data.table(mlr_resamplings)
```

The `rsmp()` function is used to construct a `Resampling` object. For example, to create a holdout strategy with a 4/5 split,

```{r}
rsmp("holdout", ratio = 0.8)
```

Or other strategies:

```{r}
# three-fold CV
cv3 = rsmp("cv", folds = 3)
# Subsampling with 3 repeats and 9/10 ratio
ss390 = rsmp("subsampling", repeats = 3, ratio = 0.9)
# 2-repeats 5-fold CV
rcv25 = rsmp("repeated_cv", repeats = 2, folds = 5)
```

The `Resampling` object created gives a definition for how the data splitting process will be performed on a task. It is possible to manually instantiate a resampling strategy using `$instantiate()`:

```{r}
cv3$instantiate(tsk_penguins)
# first 10 observations in the second training set
cv3$train_set(2)[1:10]
# first 10 observations in the third test set 
cv3$test_set(3)[1:10]
```

Note that `$instantiate()` is rarely required, as we can use the `resample()`
method to do it all.

## Resampling experiments

The `resample()` function requires a `Task`, `Learner` and `Resampling` objects. It stores the result in an `ResampleResult` object.

```{r}
rr <- resample(tsk_penguins, lrn_rpart, cv3)
rr
```

We can calculate the score for each iteration using `$score()`.^[By default,
`score()` evaluates the performance in the test sets in each iteration.]

```{r}
acc <- rr$score(msr("classif.ce"))
acc
```

The score can also be aggregated over the iterations. By default, the
`classif.ce` score uses *macro* average, where the score is calculated for
each iteration, and then averaged. The *micro* average can be computed, where
the predictions are pooled across iterations into one `Prediction` object, and
then a single score calculated.

```{r}
# the default aggregation method can be found in the $average field
msr("classif.ce")$average
# macro
rr$aggregate(msr("classif.ce"))
# micro
rr$aggregate(msr("classif.ce", average = "micro"))
```


![An example of the difference between `$score()` and `$aggregate()`](./fig/aggregation.svg)

The results can be visualised using the `autoplot.ResampleResult()` function.

```{r}
#| fig-cap: Boxplot (left) and histogram (right) of accuracy scores for 10-fold CV
#| warning: false

rr = resample(tsk_penguins, lrn_rpart, rsmp("cv", folds = 10))
gg1 <- autoplot(rr, measure = msr("classif.acc"), type = "boxplot")
gg2 <- autoplot(rr, measure = msr("classif.acc"), type = "histogram")
gg1 + gg2
```

## ResampleResult objects

In general, the intermediate models produced for each iterations are not stored
in the `ResampleResult` object, to save memory. However, we can configure the
`resample()` function to keep these models.

```{r}
#| output: false
rr <- resample(tsk_penguins, lrn_rpart, cv3, store_models = TRUE)
```

:::{.panel-tabset}

## Model 1
```{r}
rr$learners[[1]]
rr$learners[[1]]$model
```
## Model 2
```{r}
rr$learners[[2]]
rr$learners[[2]]$model
```
## Model 3
```{r}
rr$learners[[3]]
rr$learners[[3]]$model
```
:::

We can also use this to inspect the most important variables in each iteration to help us learn more about the models.

```{r}
# print 2nd and 3rd iteration
lapply(rr$learners, function(x) x$model$variable.importance)
```

## Custom resampling

If custom resampling is required, then the `rsmp("custom")` function can be used. See [Section 3.2.4](https://mlr3book.mlr-org.com/chapters/chapter3/evaluation_and_benchmarking.html#sec-resamp-custom) of the {mlr3} tutorial to learn more.

## Stratification and grouping

It is possible to group or stratify observations. Grouped sampling can be used to ensure that all observations of the same group (*e.g.* country) belong to either the training or test set, to prevent leaking information. Stratified sampling ensures that one or more discrete features within the training of test sets will have a similar distribution an in the original task containing all observations.

For information, see [Section 3.2.5](https://mlr3book.mlr-org.com/chapters/chapter3/evaluation_and_benchmarking.html#sec-strat-group) of the {mlr3} tutorial.

# Benchmarking {#sec-benchmarking}

Benchmarking
: the comparison of different learners on one or more tasks.

## `benchmark()`

The `benchmark()` function runs `resample()` on each task and learner separately and collates the results.

To use `benchmark()` we first need to call `benchmark_grid()`, which constructs an exhaustive design to describe all combinations of the learners, tasks and resamplings to be used in a benchmark experiment.^[By default, `benchmark_grid()` instantiates the resamplings on the tasks, which
means that concrete train-test splits are generated. Since this process is
stochastic, it is necessary to set a seed before calling `benchmark_grid()` to
ensure reproducibility of the data splits.]

```{r}
tasks = tsks(c("german_credit", "sonar"))
# 'classif.ranger' is a random classification forest
learners = lrns(c("classif.rpart", "classif.ranger",
  "classif.featureless"), predict_type = "prob")
rsmp_cv5 = rsmp("cv", folds = 5)

design = benchmark_grid(tasks, learners, rsmp_cv5)
design
```

Then we can run and experiement on the benchmark design.

```{r}
#| echo: false
#| output: false

bmr = benchmark(design)
```
```r
bmr = benchmark(design)
bmr
```
```{r}
#| echo: false
bmr
```

As `benchmark()` is an extensive of `resample()`, we can obtain the score, and
aggregate in the same way.

```{r}
bmr$score()[, .(iteration, task_id, learner_id, classif.ce)]
bmr$aggregate()[, .(task_id, learner_id, classif.ce)]
```

Here, we can possibly conclude that the random forest is the best of all three
models on each task, although some analysis will have to be done to assert
this confidently.^[How to do this analysis comes much later in the {mlr3} [tutorial](https://mlr3book.mlr-org.com/chapters/chapter11/large-scale_benchmarking.html#sec-benchmark-analysis)]

## BenchmarkResult objects
A `BenchmarkResult` object is a collection of multiple `ResampleResult` objects.

# Evaluating Binary Classifiers {#sec-binary-classification}
