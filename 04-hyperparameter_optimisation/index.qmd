---
title: "Applied Machine Learning Using mlr3"
subtitle: "Hyperparameter Optimisation"
author: "Paul Smith"
date: today
format:
  html:
    embed-resources: true
    code-annotations: hover
    grid:
      margin-width: 350px
    fig-width: 6
    fig-asp: 0.618
    toc: true
    toc-location: left
bibliography: ref.bib
reference-location: margin
citation-location: margin
fig-cap-location: margin
tbl-cap-location: margin
lst-cap-location: margin
link-citations: true
---

# Introduction and prerequisites

These notes mirror [Section 4](https://mlr3book.mlr-org.com/chapters/chapter4/hyperparameter_optimization.html) of *Applied Machine Learning Using mlr3 in R* [@becker2024hyperparameter].

Parameter
: model coefficients or weights (or other information) that are determined by the learning algorithm based on the training data.

Hyperparameter
: configures by the user and determine how the model will fit its parameters
(*i.e.* how the model is built).^[For example, the number of trees in a random
forest, penalty settings in support vectors machines *etc*]

The aim of hyperparameter optimisation (model tuning) is to find the optimal
configuration of hyperparameters of a machine learning algorithm for a given
task. There is no closed-form representation here, so it relies on resampling
with different configurations and choosing the configuration with the best
performance.

Tuner
: a hyperparameter optimisation method.

![Representation of the hyperparameter optimization loop in mlr3tuning. Blue - Hyperparameter optimization loop. Purple - Objects of the tuning instance supplied by the user. Blue-Green - Internally created objects of the tuning instance. Green - Optimization Algorithm.](./fig/hyperparameter_optimisation.svg)

```{r}
library(mlr3)
library(mlr3viz)
library(mlr3learners)
library(mlr3data)
library(mlr3tuning)
library(e1071)
library(ggplot2)
library(patchwork)
library(data.table)
options(datatable.print.nrows = 20)
```

Suppress all messaging unless it's a warning:^[The packages in `{mlr3}` that
make use of optimization, i.e., `{mlr3tuning}` or `{mlr3fselec}`t, use the
logger of their base package `{bbotk}`.]

```{r}
lgr::get_logger("mlr3")$set_threshold("warn")
lgr::get_logger("bbotk")$set_threshold("warn")
```


## Structure

This article is structured as follows:

# Model Tuning {#sec-tuning-ti}

The `{mlr3}` ecosystem uses the `{mlr3tuning}` package [@becker2025mlr3tuning] to perform hyperparameter optimisation, which has the R6 classes:

- [TuningInstanceBatchSingleCrit](https://mlr3tuning.mlr-org.com/reference/TuningInstanceBatchSingleCrit.html): a tuning 'instance' that describes the optimisation problem and stores the results;
- [TunerBatch](https://mlr3tuning.mlr-org.com/reference/TunerBatch.html): used to configure and run optimisation algorithms.

In this section, we look at optimising an SVM classifier from [{e1071}](https://cran.r-project.org/web/packages/e1071/index.html) [@meyer2024e1071] on `tsk("sonar")`.

## Learner and search space

First look at the learner's possible hyperparameters to tune, using `$param_set`.

```{r}
as.data.table(lrn("classif.svm")$param_set)[,
  .(id, class, lower, upper, nlevels)]
```

Search space (tuning space)
: the subset of hyperparameters that are chosen to tune.

Here, we will tune the numeric regularisation and kernel width hyperparameters `cost` and `gamma`.[^note1]

[^note1]: From the `svg()` [man page](https://www.rdocumentation.org/packages/e1071/versions/1.7-16/topics/svm), we can see:

    - `cost`: cost of constraints violation (default: 1)---it is the ‘C’-constant of the regularization term in the Lagrange formulation.
    - `gamma`:     parameter needed for all kernels except linear (default: 1/(data dimension))

For numerical hyperparameters, we need to set a range of values of search over, which is done using `to_tune()`.

```{r}
learner = lrn("classif.svm",
  type  = "C-classification",
  kernel = "radial",
  cost  = to_tune(1e-1, 1e5),
  gamma = to_tune(1e-1, 1)
)
learner
```

## Terminator

Terminators are stored in the `mlr_terminators` dictionary and are constructed with the sugar function `trm()`.

| Terminator           | Function Call and Default Parameters                          |
|----------------------|---------------------------------------------------------------|
| Clock Time           | `trm("clock_time")`                                            |
| Combo                | `trm("combo", any = TRUE)`                                     |
| None                 | `trm("none")`                                                  |
| Number of Evaluations| `trm("evals", n_evals = 100, k = 0)`                           |
| Performance Level    | `trm("perf_reached", level = 0.1)`                             |
| Run Time             | `trm("run_time", secs = 30)`                                   |
| Stagnation           | `trm("stagnation", iters = 10, threshold = 0)`                |

: Terminators available in `{mlr3tuning}`. For an up-to-date list, see the [website](ttps://mlr-org.com/terminators.html).

## Tuning instance with `ti`

Tuning instance
: collects the tuner-agnostic information required to optimise a model (*i.e.* all information about the tuning process, except for the tuning algorithm itself).^[This includes the task to tune over, the learner to tune, the resampling method and measure used, and the terminator.] 

A tuning instance can be constructed explicitly with `ti()`, or the learner can be tuned with `tune()` (described in @sec-tuning-tune).

First, let's constuct a single-objective tuning problem (*i.e.* tuning over one
measure), by using `ti()` to create a `TuningInstanceBatchSingleCrit`. We will
use three-fold CV and optimise the classification error measure. We will use
the grid search tuner, so can use `trm("none")` as we want to iterate over the
full grid without stopping too soon.

```{r}
tsk_sonar <- tsk("sonar")
learner <- lrn("classif.svm",
               cost = to_tune(1e-1, 1e5),
               gamma = to_tune(1e-1, 1),
               kernel = "radial",
               type = "C-classification"
               )

instance <- ti(
               task = tsk_sonar,
               learner = learner,
               resampling = rsmp("cv", folds = 3),
               measures = msr("classif.ce"),
               terminator = trm("none")
               )

instance
```


## Tuner

Now we need to decide how to tune the model, which is done using a `Tuner` class.

| Tuner                          | Function Call           | Package         |
|-------------------------------|--------------------------|-----------------|
| Random Search                 | `tnr("random_search")`   | mlr3tuning      |
| Grid Search                   | `tnr("grid_search")`     | mlr3tuning      |
| Bayesian Optimization         | `tnr("mbo")`             | mlr3mbo         |
| CMA-ES                        | `tnr("cmaes")`           | adagio          |
| Iterated Racing               | `tnr("irace")`           | irace           |
| Hyperband                    | `tnr("hyperband")`       | mlr3hyperband   |
| Generalized Simulated Annealing | `tnr("gensa")`         | GenSA           |
| Nonlinear Optimization        | `tnr("nloptr")`          | nloptr          |

: Tuning algorithms available in `{mlr3tuning}`. For a complete list, see the [website](https://mlr-org.com/tuners.html).

Tuning can be either via *search strategies* (*e.g.* grid, or random search) or
*adaptive algorithms* (*e.g.* Bayesian optimisation, CMA-ES^[which is an
example of an evolutionary strategy]). For more information about these
methods, see [Section
4.1.4](https://mlr3book.mlr-org.com/chapters/chapter4/hyperparameter_optimization.html#sec-tuner)
or the mlr3 tutorial.

### Choosing strategies

In low dimensions, often grid search may be used to exhaustively evaluate the
search space in a reasonable time. However, when the search space gets large
this is infeasible. From [Section
4.1.4](https://mlr3book.mlr-org.com/chapters/chapter4/hyperparameter_optimization.html#sec-tuner):

> If hyperparameter configurations can be evaluated quickly, evolutionary
strategies often work well. On the other hand, if model evaluations are
time-consuming and the optimization budget is limited, Bayesian optimization is
usually preferred, as it is quite sample efficient compared to other
algorithms, i.e., less function evaluations are needed to find good
configurations. Hence, Bayesian optimization is usually recommended for HPO. 

For the SVM example, we will use a grid search, with a $5 \times 5$ grid.

```{r}
tuner = tnr("grid_search", resolution = 5, batch_size = 10)
tuner
```

The `resolution` and `batch_size` parameters are *control* parameters, specific to the grid search tuner.^[The `batch_size` parameter controls how many configurations are evaluated at the same time when parallisation is enables (see [Section 10.1.3](https://mlr3book.mlr-org.com/chapters/chapter10/advanced_technical_aspects_of_mlr3.html#sec-parallel-tuning))]

```{r}
tuner$param_set
```

#### Triggering the tuning process

Now we can start the tuning process. We pass the contructed `TuningInstanceBatchSingleCrit` to the `$optimize()` method of the initialised `TunerBatch`, which triggers the hyperparameter optimisation loop.

```{r}
tuner$optimize(instance)
```

The optimizer returns the best hyperparameter configuration and the
corresponding performance, this information is also stored in
`instance$result`. The `$learner_param_vals` field of the `$result` lists the
optimal hyperparameters from tuning, as well as any other hyperparameters that
were set.

```{r}
#| lst-label: lst-instant-result
#| lst-cap: Parameter values after tuning using a grid search.

instance$result
```

```{r}
instance$result$learner_param_vals
```


The `$x_domain` field is useful for hyperparameter transformation, which we look at briefly now (@sec-hyper-transformations).

### Logarithmic transformations {#sec-hyper-transformations}

If a hyperparameter has a large upper bound, tuning on a logarithmic scale can
be more efficient than on a linear scale. After transform, the majority of the
points are small, with a few being very large.

```r
cost = runif(1000, log(1e-5), log(1e5))
exp_cost = exp(cost)
```

::: {layout-ncol=2}

![](./fig/log_scale.png)

![](./fig/log_scale2.png)

Before (left) and after log-transform (right).
:::

```{r}
#| lst-label: lst-instant-result-log
#| lst-cap: Parameter values after tuning using a grid search and log-transform.

learner = lrn("classif.svm",
  cost  = to_tune(1e-5, 1e5, logscale = TRUE),
  gamma = to_tune(1e-5, 1e5, logscale = TRUE),
  kernel = "radial",
  type = "C-classification"
)

instance = ti(
  task = tsk_sonar,
  learner = learner,
  resampling = rsmp("cv", folds = 3),
  measures = msr("classif.ce"),
  terminator = trm("none")
)

tuner$optimize(instance)
```

This log-transform improved the hyperparameter search, as `classif.ce` is
smaller than in @lst-instant-result. Note that the optimal parameters shown in
@lst-instant-result-log are pre-transformation. To see the values after
transformation -- that is, $e^x$ --  use `$x_domain`.

```{r}
instance$result$x_domain
```

More complex transformations are discussed in @sec-advanced-search-space.


## Analysing and using the result

We can examine all the hyperparameter configurations that were evaluated, using `$archive`.

```{r}
as.data.table(instance$archive)[, .(cost, gamma, classif.ce)]
```

Here, the columns show the tested configurations before transformation.^[That
is why some are negative, even though cost and gamma values must be positive.
E.g. $-11.512925 \approx \log(1\times 10^{-5})$] We can
also look at other features, for example, time of evaluation, model runtime
and any errors or warnings.

```{r}
as.data.table(instance$archive)[,
  .(timestamp, runtime_learners, errors, warnings)]
```

All the resamplings combined (as a `BenchmarkResult` object) can be seen using `instance$archive$benchmark_result`

```{r}
instance$archive$benchmark_result
```

The visualisation of these results can be seen in @fig-instance-surface (which
uses
[autoplot.TuningInstanceBatchSingleCrit](https://mlr3viz.mlr-org.com/reference/autoplot.TuningInstanceBatchSingleCrit.html))

```{r}
#| fig-cap: Model performance with different configurations for cost and gamma. Bright yellow regions represent the model performing worse and dark blue performing better.
#| label: fig-instance-surface

autoplot(instance, type = "surface")
```

### Training an optimised model

After we have found *good* hyperparameters for our learner through tuning, we can train the final model on the data.
First, construct the learner with the 'optimal' hyperparameters.

```{r}
lrn_svm_tuned = lrn("classif.svm")
lrn_svm_tuned$param_set$values = instance$result_learner_param_vals
lrn_svm_tuned
```

And then train this learner on the data.

```{r}
lrn_svm_tuned$train(tsk_sonar)$model
```

## Tuning with `tune` and `auto_tuner` {#sec-tuning-tune}

In @sec-tuning-ti we created a tuning instance (using `ti()`), passed this to
the tuner, and then called `$optimize()` to start the tuning process.
There are two helper methods in `{mlr3tuning}` to simplify this process --
`tune()` and `auto_tuner()`.

The `tune()` function creates the tuning instance and calls `$optimize()`. That
is, it skips the step of calling `tuner$optimize(instance)`.

```{r}
tnr_grid_search = tnr("grid_search", resolution = 5, batch_size = 5)
lrn_svm = lrn("classif.svm",
  cost  = to_tune(1e-5, 1e5, logscale = TRUE),
  gamma = to_tune(1e-5, 1e5, logscale = TRUE),
  kernel = "radial",
  type = "C-classification"
)
rsmp_cv5 = rsmp("cv", folds = 5)
msr_ce = msr("classif.ce")

set.seed(10)
instance = mlr3tuning::tune( #<1>
  tuner = tnr_grid_search,
  task = tsk_sonar,
  learner = lrn_svm,
  resampling = rsmp_cv5,
  measures = msr_ce
)
instance$result
```
1. I have specificed the package here (`mlr3tuning::`), as there is also the function `e1071::tune()` and thus a namespace conflict.

:::{.column-margin}
The equivalent code using `ti()` involves specifying the tuner instance first and then triggering it.

```{r}
tuner = tnr("grid_search", resolution = 5, batch_size = 5)
instance <- ti(
               task = tsk_sonar,
               learner = lrn_svm,
               resampling = rsmp_cv5,
               measures = msr_ce,
               terminator = trm("none")
               )

set.seed(10)
tuner$optimize(instance)
instance$result
```

:::

The `auto_tuner()` helper function creates an object of class
[AutoTuner](https://mlr3tuning.mlr-org.com/reference/AutoTuner.html).
It inherits from the `Learner` class and wraps the information needed for tuning, so that you can treat a learner waiting to be optimised just like any other learner.
The `AutoTuner` runs `tune()` on the data that is passed to the model when `$train()` is called.

![Illustration of an Auto-Tuner](./fig/autotuner.svg){width=50% fig-align="center"}

```{r}
#| lst-label: lst-auto-tuner
#| lst-cap: Initialising the `auto_tuner()`

at = auto_tuner(
  tuner = tnr_grid_search,
  learner = lrn_svm,
  resampling = rsmp_cv5,
  measure = msr_ce
)

at
```

Now, we can call `$train()`, which will first tune the hyperparameters in the
search space given in the output from @lst-auto-tuner, and then fit the optimal
model.

```{r}
set.seed(10)

split <- partition(tsk_sonar)
at$train(tsk_sonar, row_ids = split$train)
at$predict(tsk_sonar, row_ids = split$test)$score()
```

We can also look at the tuning instance, as before.

```{r}
at$tuning_instance
at$tuning_instance$result
```

Now, in @sec-nested-resampling below will discuss *nested resampling*, which involves passing the `AutoTuner` to `resample()` and `benchmark()`.

# Nested Resampling {#sec-nested-resampling}

Analogously to *optimism* when obtaining the predictive ability of a model
trained on the same data set [see @harrell2001regression, Section 5.3.4], with
HPO we also have to consider the bias due to
the same data being used for determining the optimal configuration and the
evaluation of the resulting model. Therefore, we need to do additional
resampling to reduce this bias when evaluating the performance of a model.

Nested resampling
: a method to seperate the model optimisation from the process of estimating
the performance of the tuned model, using additional resampling.^[The is, while
model performance is estimated using a resampling method in the ‘usual way’,
tuning is then performed by resampling the resampled data]

![An illustration of nested resampling. The large blocks represent three-fold CV for the outer resampling for model evaluation and the small blocks represent four-fold CV for the inner resampling for HPO. The light blue blocks are the training sets and the dark blue blocks are the test sets.](./fig/nested_resampling.svg){#fig-nested-resampling-example}

@fig-nested-resampling-example shows the following nested resampling example:

1. Outer resampling start
   - Perform 3-fold cross-validation on the full dataset.
   - For each outer fold, split the data into:
     - Training set (light blue blocks)
     - Test set (dark blue block)

2. Inner resampling
   - Within each outer training set, perform 4-fold cross-validation.
   - These inner folds are used for tuning hyperparameters (not evaluation).

3. HPO – Hyperparameter tuning
   - Use the inner training and validation sets to evaluate different hyperparameter combinations.
   - Select the best hyperparameter configuration based on performance across inner folds.

4. Training
   - Fit the model on the entire outer training set using the tuned hyperparameters.

5. Evaluation
   - Evaluate the trained model on the outer test set (unseen during tuning).

6. Outer resampling repeats
   - Repeat steps 2–5 for each of the 3 outer folds.

7. Aggregation
   - Average the 3 outer test performance scores.
   - This gives an unbiased estimate of the model’s generalisation performance with tuning.


From the `{mlr3}` tutorial ([Section
4.3](https://mlr3book.mlr-org.com/chapters/chapter4/hyperparameter_optimization.html#sec-nested-resampling)), 

> A common mistake is to think of nested resampling as a method to select
optimal model configurations. Nested resampling is a method to compare models
and to estimate the generalization performance of a tuned model, however, this
is the performance based on multiple different configurations (one from each
outer fold) and not performance based on a single configuration (Section
4.3.2). If you are interested in identifying optimal configurations, then use
`tune()`/`ti()` or `auto_tuner()` with `$train()` on the complete
dataset.^[**Key Point**: Nested resampling is not for selecting the “final”
best hyperparameter configuration. Instead, it’s used to get an unbiased
estimate of model performance after tuning — taking into account the tuning
process itself.]

:::{.callout-note}
# This is computationally intensive

Jacob Fiksel has a [nice post](https://jfiksel.github.io/2022-01-04-missdat/)
about how this can get very computationally expensive very quickly.

It is recommended to use penalisation here, if possible. This is discussed in
[Section
10.1](https://mlr3book.mlr-org.com/chapters/chapter10/advanced_technical_aspects_of_mlr3.html#sec-parallelization)
of the `{mlr3}` tutorial.

:::

## Nested resampling with `AutoTuner`

Nested resampling is all altomated in `{mlr3tuning}` by passing `AutoTuner` to `resample()` or `benchmark()`.

Let's continue with the previous example, using the auto-tuner to resample a SVC with three-fold CV in the outer resampling and four-fold CV in the inner resampling.
The `tnr_grid_search` and `lrn_svm` objects were create in @sec-tuning-tune.

```{r}
at <- auto_tuner(
                 tuner = tnr_grid_search,
                 learner = lrn_svm,
                 resampling = rsmp("cv", folds = 4),
                 measure = msr_ce
                 )

rr <- resample(tsk_sonar, at, rsmp("cv", folds = 3), store_models = TRUE) #<1>
rr
```
1. Setting `store_models = TRUE` allows us to see the `AutoTuner` models
   (fitted on the outer training data), and also enables investigation of the inner tuning instances.

The estimated performance of the tuned model is the aggregated performance of all outer resampling iterations.

```{r}
rr$aggregate()
```

The function `extract_inner_tuning_results()` and
`extract_inner_tuning_archives()` return the optimal configurations (across all
outer folds) and full tuning archives, respectively.
There are $75$ rows for `extract_inner_tuning_archives()` as each of the
3 outer folds triggers a full 25-point grid search. Therefore, $3 \times 25
= 75$ tuning evaluations are recorded.

```{r}
extract_inner_tuning_results(rr)[,
  .(iteration, cost, gamma, classif.ce)]

extract_inner_tuning_archives(rr)[,
  .(iteration, cost, gamma, classif.ce)]
```

In [Section
4.3.2](https://mlr3book.mlr-org.com/chapters/chapter4/hyperparameter_optimization.html#sec-resample-overfitting),
nested resampling is compared to a normal tuning technique, to show that the
latter gives much more optimistically biased performance estimates.

:::{.callout-important}
# Summary of how to use HPO and nested resampling

**Why use tuned hyperparameters on the full dataset?**

After tuning (e.g., via `auto_tuner()` or `tune()`), you should train your final model using all available training data. This ensures:

- The model benefits from the maximum amount of information.
- You're using the best hyperparameters found during tuning.
- This final model is the one you'd use for deployment or making future predictions.

**Why report performance from nested resampling?**

Nested resampling gives an unbiased estimate of generalisation error because:

- The tuning is performed inside the training folds (inner resampling).
- Evaluation is done on outer test folds that were never used for tuning.
- This corrects for optimism — the bias introduced when performance is evaluated on data also used for tuning.

**Summary:**

<div style="text-align: center;">
  <div style="border: 1px solid #ccc; padding: 0.5em; display: inline-block; text-align: left;">

  | Step                            | Purpose                                          |
  |---------------------------------|--------------------------------------------------|
  | Train tuned model on full data  | For final model and deployment                   |
  | Report nested CV performance    | For honest estimate of generalisation error      |

  </div>
</div>

:::

# More Advanced Search Spaces {#sec-advanced-search-space}

In this section, we look at the following:

- Using `to_tune()` to tune different scalar parameter classes;
- Using `ParamSet` to define your own search space (e.g. to handle tuning over vectors, transformations or handling parameter dependencies);
- Accessing a database of standardised search spaces from the literature.

## Scalar parameter tuning

The `to_tune()` function can be used to tune parameters of any class.

```{r}
learner = lrn("classif.svm",
  cost  = to_tune(1e-1, 1e5),
  kernel = to_tune(c("radial", "linear")), #<1>
  shrinking = to_tune(), #<2>
  type = "C-classification"
)

learner$param_set$search_space()
```
1. A factor, so pass the vector of levels required.
2. A logical, which `to_tune()` recognises automatically.

Numerical parameters can also be treated as factors if we want to discretise them over a small subset of possible values. For example;
```r
lrn("classif.ranger", num.trees = to_tune(c(100, 200, 400)))
```

## Defining Search Spaces with `ps`

The helper function `to_tune()` creates a parameter set that is then used by `tune()`, `ti()` or `auto_tuner()`.
However, sometimes you will need to create a parameter set manually using `ps()`.
This function takes names arguments of class `Domain`, which can be create using the sugar functions in the table below.

| Constructor | Description                            | Underlying Class |
|-------------|----------------------------------------|------------------|
| `p_dbl`     | Real valued parameter (“double”)       | `ParamDbl`       |
| `p_int`     | Integer parameter                      | `ParamInt`       |
| `p_fct`     | Discrete valued parameter (“factor”)   | `ParamFct`       |
| `p_lgl`     | Logical / Boolean parameter            | `ParamLgl`       |
| `p_uty`     | Untyped parameter                      | `ParamUty`       |
:`Domain` Constructors and their resulting `Domain`. 

A simple example: create a search space to tune `cost` `kernel` and `shrinking`,

```{r}
search_space = ps(
                  cost = p_dbl(lower = 1e-1, upper = 1e5),
                  kernel = p_fct(c("radial", "linear")),
                  shrinking = p_lgl()
                  )
```
We then pass this search space into `ti()`.

```{r}
ti(
   tsk_sonar, 
   lrn("classif.svm", type = "C-classification"),
   rsmp("cv", folds = 3),
   msr_ce,
   trm("none"),
   search_space = search_space
   )
```

For creating more advanced search spaces, see [Section 4.4.3](https://mlr3book.mlr-org.com/chapters/chapter4/hyperparameter_optimization.html#sec-tune-trafo) (*Transformations and Tuning Over Vectors*).^[This section considers different transformations of parameters, and scenarios where hyperparameters are interdependent or need to be transformed jointly. For instance, when tuning a support vector machine (SVM), the cost and gamma parameters often require simultaneous optimization due to their combined effect on model performance.]

# Conclusion

| Class                                                 | Constructor/Function               | Fields/Methods                          |
|--------------------------------------------------------|------------------------------------|------------------------------------------|
| `Terminator`                                           | `trm()`                            | –                                        |
| `TuningInstanceBatchSingleCrit` or `TuningInstanceBatchMultiCrit` | `ti()` / `tune()`             | `$result`; `$archive`                   |
| `TunerBatch`                                           | `tnr()`                            | `$optimize()`                           |
| `TuneToken`                                            | `to_tune()`                        | –                                        |
| `AutoTuner`                                            | `auto_tuner()`                     | `$train()`; `$predict()`; `$tuning_instance` |
| –                                                      | `extract_inner_tuning_results()`  | –                                        |
| –                                                      | `extract_inner_tuning_archives()` | –                                        |
| `ParamSet`                                             | `ps()`                             | –                                        |
| `TuningSpace`                                          | `lts()`                            | `$values`                                |
:Important classes and functions covered in this chapter with underlying class (if applicable), class constructor or function, and important class fields and methods (if applicable).
