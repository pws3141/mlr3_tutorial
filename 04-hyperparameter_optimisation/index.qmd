---
title: "Applied Machine Learning Using mlr3"
subtitle: "Hyperparameter Optimisation"
author: "Paul Smith"
date: today
format:
  html:
    embed-resources: true
    code-annotations: hover
    grid:
      margin-width: 350px
    fig-width: 6
    fig-asp: 0.618
    toc: true
    toc-location: left
bibliography: ref.bib
reference-location: margin
citation-location: margin
fig-cap-location: margin
tbl-cap-location: margin
lst-cap-location: margin
link-citations: true
---

# Introduction and prerequisites

These notes mirror [Section 4](https://mlr3book.mlr-org.com/chapters/chapter4/hyperparameter_optimization.html) of *Applied Machine Learning Using mlr3 in R* [@becker2024hyperparameter].

Parameter
: model coefficients or weights (or other information) that are determined by the learning algorithm based on the training data.

Hyperparameter
: configures by the user and determine how the model will fit its parameters
(*i.e.* how the model is built).^[For example, the number of trees in a random
forest, penalty settings in support vectors machines *etc*]

The aim of hyperparameter optimisation (model tuning) is to find the optimal
configuration of hyperparameters of a machine learning algorithm for a given
task. There is no closed-form representation here, so it relies on resampling
with different configurations and choosing the configuration with the best
performance.

Tuner
: a hyperparameter optimisation method.

![Representation of the hyperparameter optimization loop in mlr3tuning. Blue - Hyperparameter optimization loop. Purple - Objects of the tuning instance supplied by the user. Blue-Green - Internally created objects of the tuning instance. Green - Optimization Algorithm.](./fig/hyperparameter_optimisation.svg)

```{r}
#| cache: false

library(e1071)
library(mlr3)
library(mlr3viz)
library(mlr3learners)
library(mlr3data)
library(mlr3tuning)
library(mlr3tuningspaces)
library(ggplot2)
library(patchwork)
library(data.table)
options(datatable.print.nrows = 20)
```

Suppress all messaging unless it's a warning:^[The packages in `{mlr3}` that
make use of optimization, i.e., `{mlr3tuning}` or `{mlr3fselec}`t, use the
logger of their base package `{bbotk}`.]

```{r}
lgr::get_logger("mlr3")$set_threshold("warn")
lgr::get_logger("bbotk")$set_threshold("warn")
```


## Structure

This article is structured as follows:

# Model Tuning {#sec-tuning-ti}

The `{mlr3}` ecosystem uses the `{mlr3tuning}` package [@becker2025mlr3tuning] to perform hyperparameter optimisation, which has the R6 classes:

- [TuningInstanceBatchSingleCrit](https://mlr3tuning.mlr-org.com/reference/TuningInstanceBatchSingleCrit.html): a tuning 'instance' that describes the optimisation problem and stores the results;
- [TunerBatch](https://mlr3tuning.mlr-org.com/reference/TunerBatch.html): used to configure and run optimisation algorithms.

In this section, we look at optimising an SVM classifier from [{e1071}](https://cran.r-project.org/web/packages/e1071/index.html) [@meyer2024e1071] on `tsk("sonar")`.

## Learner and search space

First look at the learner's possible hyperparameters to tune, using `$param_set`.

```{r}
as.data.table(lrn("classif.svm")$param_set)[,
  .(id, class, lower, upper, nlevels)]
```

Search space (tuning space)
: the subset of hyperparameters that are chosen to tune.

Here, we will tune the numeric regularisation and kernel width hyperparameters `cost` and `gamma`.[^note1]

[^note1]: From the `svg()` [man page](https://www.rdocumentation.org/packages/e1071/versions/1.7-16/topics/svm), we can see:

    - `cost`: cost of constraints violation (default: 1)---it is the ‘C’-constant of the regularization term in the Lagrange formulation.
    - `gamma`:     parameter needed for all kernels except linear (default: 1/(data dimension))

For numerical hyperparameters, we need to set a range of values of search over, which is done using `to_tune()`.

```{r}
learner = lrn("classif.svm",
  type  = "C-classification",
  kernel = "radial",
  cost  = to_tune(1e-1, 1e5),
  gamma = to_tune(1e-1, 1)
)
learner
```

## Terminator

Terminators are stored in the `mlr_terminators` dictionary and are constructed with the sugar function `trm()`.

| Terminator           | Function Call and Default Parameters                          |
|----------------------|---------------------------------------------------------------|
| Clock Time           | `trm("clock_time")`                                            |
| Combo                | `trm("combo", any = TRUE)`                                     |
| None                 | `trm("none")`                                                  |
| Number of Evaluations| `trm("evals", n_evals = 100, k = 0)`                           |
| Performance Level    | `trm("perf_reached", level = 0.1)`                             |
| Run Time             | `trm("run_time", secs = 30)`                                   |
| Stagnation           | `trm("stagnation", iters = 10, threshold = 0)`                |

: Terminators available in `{mlr3tuning}`. For an up-to-date list, see the [website](ttps://mlr-org.com/terminators.html).

## Tuning instance with `ti`

Tuning instance
: collects the tuner-agnostic information required to optimise a model (*i.e.* all information about the tuning process, except for the tuning algorithm itself).^[This includes the task to tune over, the learner to tune, the resampling method and measure used, and the terminator.] 

A tuning instance can be constructed explicitly with `ti()`, or the learner can be tuned with `tune()` (described in @sec-tuning-tune).

First, let's constuct a single-objective tuning problem (*i.e.* tuning over one
measure), by using `ti()` to create a `TuningInstanceBatchSingleCrit`. We will
use three-fold CV and optimise the classification error measure. We will use
the grid search tuner, so can use `trm("none")` as we want to iterate over the
full grid without stopping too soon.

```{r}
tsk_sonar <- tsk("sonar")
learner <- lrn("classif.svm",
               cost = to_tune(1e-1, 1e5),
               gamma = to_tune(1e-1, 1),
               kernel = "radial",
               type = "C-classification"
               )

instance <- ti(
               task = tsk_sonar,
               learner = learner,
               resampling = rsmp("cv", folds = 3),
               measures = msr("classif.ce"),
               terminator = trm("none")
               )

instance
```


## Tuner

Now we need to decide how to tune the model, which is done using a `Tuner` class.

| Tuner                          | Function Call           | Package         |
|-------------------------------|--------------------------|-----------------|
| Random Search                 | `tnr("random_search")`   | mlr3tuning      |
| Grid Search                   | `tnr("grid_search")`     | mlr3tuning      |
| Bayesian Optimization         | `tnr("mbo")`             | mlr3mbo         |
| CMA-ES                        | `tnr("cmaes")`           | adagio          |
| Iterated Racing               | `tnr("irace")`           | irace           |
| Hyperband                    | `tnr("hyperband")`       | mlr3hyperband   |
| Generalized Simulated Annealing | `tnr("gensa")`         | GenSA           |
| Nonlinear Optimization        | `tnr("nloptr")`          | nloptr          |

: Tuning algorithms available in `{mlr3tuning}`. For a complete list, see the [website](https://mlr-org.com/tuners.html).

Tuning can be either via *search strategies* (*e.g.* grid, or random search) or
*adaptive algorithms* (*e.g.* Bayesian optimisation, CMA-ES^[which is an
example of an evolutionary strategy]). For more information about these
methods, see [Section
4.1.4](https://mlr3book.mlr-org.com/chapters/chapter4/hyperparameter_optimization.html#sec-tuner)
or the mlr3 tutorial.

### Choosing strategies

In low dimensions, often grid search may be used to exhaustively evaluate the
search space in a reasonable time. However, when the search space gets large
this is infeasible. From [Section
4.1.4](https://mlr3book.mlr-org.com/chapters/chapter4/hyperparameter_optimization.html#sec-tuner):

> If hyperparameter configurations can be evaluated quickly, evolutionary
strategies often work well. On the other hand, if model evaluations are
time-consuming and the optimization budget is limited, Bayesian optimization is
usually preferred, as it is quite sample efficient compared to other
algorithms, i.e., less function evaluations are needed to find good
configurations. Hence, Bayesian optimization is usually recommended for HPO. 

For the SVM example, we will use a grid search, with a $5 \times 5$ grid.

```{r}
tuner = tnr("grid_search", resolution = 5, batch_size = 10)
tuner
```

The `resolution` and `batch_size` parameters are *control* parameters, specific to the grid search tuner.^[The `batch_size` parameter controls how many configurations are evaluated at the same time when parallisation is enables (see [Section 10.1.3](https://mlr3book.mlr-org.com/chapters/chapter10/advanced_technical_aspects_of_mlr3.html#sec-parallel-tuning))]

```{r}
tuner$param_set
```

#### Triggering the tuning process

Now we can start the tuning process. We pass the contructed `TuningInstanceBatchSingleCrit` to the `$optimize()` method of the initialised `TunerBatch`, which triggers the hyperparameter optimisation loop.

```{r}
tuner$optimize(instance)
```

The optimizer returns the best hyperparameter configuration and the
corresponding performance, this information is also stored in
`instance$result`. The `$learner_param_vals` field of the `$result` lists the
optimal hyperparameters from tuning, as well as any other hyperparameters that
were set.

```{r}
#| lst-label: lst-instant-result
#| lst-cap: Parameter values after tuning using a grid search.

instance$result
```

```{r}
instance$result$learner_param_vals
```


The `$x_domain` field is useful for hyperparameter transformation, which we look at briefly now (@sec-hyper-transformations).

### Logarithmic transformations {#sec-hyper-transformations}

If a hyperparameter has a large upper bound, tuning on a logarithmic scale can
be more efficient than on a linear scale. After transform, the majority of the
points are small, with a few being very large.

```r
cost = runif(1000, log(1e-5), log(1e5))
exp_cost = exp(cost)
```

::: {layout-ncol=2}

![](./fig/log_scale.png)

![](./fig/log_scale2.png)

Before (left) and after log-transform (right).
:::

```{r}
#| lst-label: lst-instant-result-log
#| lst-cap: Parameter values after tuning using a grid search and log-transform.

learner = lrn("classif.svm",
  cost  = to_tune(1e-5, 1e5, logscale = TRUE),
  gamma = to_tune(1e-5, 1e5, logscale = TRUE),
  kernel = "radial",
  type = "C-classification"
)

instance = ti(
  task = tsk_sonar,
  learner = learner,
  resampling = rsmp("cv", folds = 3),
  measures = msr("classif.ce"),
  terminator = trm("none")
)

tuner$optimize(instance)
```

This log-transform improved the hyperparameter search, as `classif.ce` is
smaller than in @lst-instant-result. Note that the optimal parameters shown in
@lst-instant-result-log are pre-transformation. To see the values after
transformation -- that is, $e^x$ --  use `$x_domain`.

```{r}
instance$result$x_domain
```

More complex transformations are discussed in @sec-advanced-search-space.


## Analysing and using the result

We can examine all the hyperparameter configurations that were evaluated, using `$archive`.

```{r}
as.data.table(instance$archive)[, .(cost, gamma, classif.ce)]
```

Here, the columns show the tested configurations before transformation.^[That
is why some are negative, even though cost and gamma values must be positive.
E.g. $-11.512925 \approx \log(1\times 10^{-5})$] We can
also look at other features, for example, time of evaluation, model runtime
and any errors or warnings.

```{r}
as.data.table(instance$archive)[,
  .(timestamp, runtime_learners, errors, warnings)]
```

All the resamplings combined (as a `BenchmarkResult` object) can be seen using `instance$archive$benchmark_result`

```{r}
instance$archive$benchmark_result
```

The visualisation of these results can be seen in @fig-instance-surface (which
uses
[autoplot.TuningInstanceBatchSingleCrit](https://mlr3viz.mlr-org.com/reference/autoplot.TuningInstanceBatchSingleCrit.html))

```{r}
#| fig-cap: Model performance with different configurations for cost and gamma. Bright yellow regions represent the model performing worse and dark blue performing better.
#| label: fig-instance-surface

autoplot(instance, type = "surface")
```

### Training an optimised model

After we have found *good* hyperparameters for our learner through tuning, we can train the final model on the data.
First, construct the learner with the 'optimal' hyperparameters.

```{r}
lrn_svm_tuned = lrn("classif.svm")
lrn_svm_tuned$param_set$values = instance$result_learner_param_vals
lrn_svm_tuned
```

And then train this learner on the data.

```{r}
lrn_svm_tuned$train(tsk_sonar)$model
```

## Tuning with `tune` and `auto_tuner` {#sec-tuning-tune}

In @sec-tuning-ti we created a tuning instance (using `ti()`), passed this to
the tuner, and then called `$optimize()` to start the tuning process.
There are two helper methods in `{mlr3tuning}` to simplify this process --
`tune()` and `auto_tuner()`.

The `tune()` function creates the tuning instance and calls `$optimize()`. That
is, it skips the step of calling `tuner$optimize(instance)`.

```{r}
tnr_grid_search = tnr("grid_search", resolution = 5, batch_size = 5)
lrn_svm = lrn("classif.svm",
  cost  = to_tune(1e-5, 1e5, logscale = TRUE),
  gamma = to_tune(1e-5, 1e5, logscale = TRUE),
  kernel = "radial",
  type = "C-classification"
)
rsmp_cv5 = rsmp("cv", folds = 5)
msr_ce = msr("classif.ce")

set.seed(10)
instance = mlr3tuning::tune( #<1>
  tuner = tnr_grid_search,
  task = tsk_sonar,
  learner = lrn_svm,
  resampling = rsmp_cv5,
  measures = msr_ce
)
instance$result
```
1. I have specificed the package here (`mlr3tuning::`), as there is also the function `e1071::tune()` and thus a namespace conflict.

:::{.column-margin}
The equivalent code using `ti()` involves specifying the tuner instance first and then triggering it.

```{r}
tuner = tnr("grid_search", resolution = 5, batch_size = 5)
instance <- ti(
               task = tsk_sonar,
               learner = lrn_svm,
               resampling = rsmp_cv5,
               measures = msr_ce,
               terminator = trm("none")
               )

set.seed(10)
tuner$optimize(instance)
instance$result
```

:::

The `auto_tuner()` helper function creates an object of class
[AutoTuner](https://mlr3tuning.mlr-org.com/reference/AutoTuner.html).
It inherits from the `Learner` class and wraps the information needed for tuning, so that you can treat a learner waiting to be optimised just like any other learner.
The `AutoTuner` runs `tune()` on the data that is passed to the model when `$train()` is called.

![Illustration of an Auto-Tuner](./fig/autotuner.svg){width=50% fig-align="center"}

```{r}
#| lst-label: lst-auto-tuner
#| lst-cap: Initialising the `auto_tuner()`

at = auto_tuner(
  tuner = tnr_grid_search,
  learner = lrn_svm,
  resampling = rsmp_cv5,
  measure = msr_ce
)

at
```

Now, we can call `$train()`, which will first tune the hyperparameters in the
search space given in the output from @lst-auto-tuner, and then fit the optimal
model.

```{r}
set.seed(10)

split <- partition(tsk_sonar)
at$train(tsk_sonar, row_ids = split$train)
at$predict(tsk_sonar, row_ids = split$test)$score()
```

We can also look at the tuning instance, as before.

```{r}
at$tuning_instance
at$tuning_instance$result
```

Now, in @sec-nested-resampling below will discuss *nested resampling*, which involves passing the `AutoTuner` to `resample()` and `benchmark()`.

# Nested Resampling {#sec-nested-resampling}

Analogously to *optimism* when obtaining the predictive ability of a model
trained on the same data set [see @harrell2001regression, Section 5.3.4], with
HPO we also have to consider the bias due to
the same data being used for determining the optimal configuration and the
evaluation of the resulting model. Therefore, we need to do additional
resampling to reduce this bias when evaluating the performance of a model.

Nested resampling
: a method to seperate the model optimisation from the process of estimating
the performance of the tuned model, using additional resampling.^[The is, while
model performance is estimated using a resampling method in the ‘usual way’,
tuning is then performed by resampling the resampled data]

![An illustration of nested resampling. The large blocks represent three-fold CV for the outer resampling for model evaluation and the small blocks represent four-fold CV for the inner resampling for HPO. The light blue blocks are the training sets and the dark blue blocks are the test sets.](./fig/nested_resampling.svg){#fig-nested-resampling-example}

@fig-nested-resampling-example shows the following nested resampling example:

1. Outer resampling start
   - Perform 3-fold cross-validation on the full dataset.
   - For each outer fold, split the data into:
     - Training set (light blue blocks)
     - Test set (dark blue block)

2. Inner resampling
   - Within each outer training set, perform 4-fold cross-validation.
   - These inner folds are used for tuning hyperparameters (not evaluation).

3. HPO – Hyperparameter tuning
   - Use the inner training and validation sets to evaluate different hyperparameter combinations.
   - Select the best hyperparameter configuration based on performance across inner folds.

4. Training
   - Fit the model on the entire outer training set using the tuned hyperparameters.

5. Evaluation
   - Evaluate the trained model on the outer test set (unseen during tuning).

6. Outer resampling repeats
   - Repeat steps 2–5 for each of the 3 outer folds.

7. Aggregation
   - Average the 3 outer test performance scores.
   - This gives an unbiased estimate of the model’s generalisation performance with tuning.


From the `{mlr3}` tutorial ([Section
4.3](https://mlr3book.mlr-org.com/chapters/chapter4/hyperparameter_optimization.html#sec-nested-resampling)), 

> A common mistake is to think of nested resampling as a method to select
optimal model configurations. Nested resampling is a method to compare models
and to estimate the generalization performance of a tuned model, however, this
is the performance based on multiple different configurations (one from each
outer fold) and not performance based on a single configuration (Section
4.3.2). If you are interested in identifying optimal configurations, then use
`tune()`/`ti()` or `auto_tuner()` with `$train()` on the complete
dataset.^[**Key Point**: Nested resampling is not for selecting the “final”
best hyperparameter configuration. Instead, it’s used to get an unbiased
estimate of model performance after tuning — taking into account the tuning
process itself.]

:::{.callout-note}
# This is computationally intensive

Jacob Fiksel has a [nice post](https://jfiksel.github.io/2022-01-04-missdat/)
about how this can get very computationally expensive very quickly.

It is recommended to use penalisation here, if possible. This is discussed in
[Section
10.1](https://mlr3book.mlr-org.com/chapters/chapter10/advanced_technical_aspects_of_mlr3.html#sec-parallelization)
of the `{mlr3}` tutorial.

:::

## Nested resampling with `AutoTuner`

Nested resampling is all altomated in `{mlr3tuning}` by passing `AutoTuner` to `resample()` or `benchmark()`.

Let's continue with the previous example, using the auto-tuner to resample a SVC with three-fold CV in the outer resampling and four-fold CV in the inner resampling.
The `tnr_grid_search` and `lrn_svm` objects were create in @sec-tuning-tune.

```{r}
at <- auto_tuner(
                 tuner = tnr_grid_search,
                 learner = lrn_svm,
                 resampling = rsmp("cv", folds = 4),
                 measure = msr_ce
                 )

rr <- resample(tsk_sonar, at, rsmp("cv", folds = 3), store_models = TRUE) #<1>
rr
```
1. Setting `store_models = TRUE` allows us to see the `AutoTuner` models
   (fitted on the outer training data), and also enables investigation of the inner tuning instances.

The estimated performance of the tuned model is the aggregated performance of all outer resampling iterations.

```{r}
rr$aggregate()
```

The function `extract_inner_tuning_results()` and
`extract_inner_tuning_archives()` return the optimal configurations (across all
outer folds) and full tuning archives, respectively.
There are $75$ rows for `extract_inner_tuning_archives()` as each of the
3 outer folds triggers a full 25-point grid search. Therefore, $3 \times 25
= 75$ tuning evaluations are recorded.

```{r}
extract_inner_tuning_results(rr)[,
  .(iteration, cost, gamma, classif.ce)]

extract_inner_tuning_archives(rr)[,
  .(iteration, cost, gamma, classif.ce)]
```

In [Section
4.3.2](https://mlr3book.mlr-org.com/chapters/chapter4/hyperparameter_optimization.html#sec-resample-overfitting),
nested resampling is compared to a normal tuning technique, to show that the
latter gives much more optimistically biased performance estimates.

:::{.callout-important}
# Summary of how to use HPO and nested resampling

**Why use tuned hyperparameters on the full dataset?**

After tuning (e.g., via `auto_tuner()` or `tune()`), you should train your final model using all available training data. This ensures:

- The model benefits from the maximum amount of information.
- You're using the best hyperparameters found during tuning.
- This final model is the one you'd use for deployment or making future predictions.

**Why report performance from nested resampling?**

Nested resampling gives an unbiased estimate of generalisation error because:

- The tuning is performed inside the training folds (inner resampling).
- Evaluation is done on outer test folds that were never used for tuning.
- This corrects for optimism — the bias introduced when performance is evaluated on data also used for tuning.

**Summary:**

<div style="text-align: center;">
  <div style="border: 1px solid #ccc; padding: 0.5em; display: inline-block; text-align: left;">

  | Step                            | Purpose                                          |
  |---------------------------------|--------------------------------------------------|
  | Train tuned model on full data  | For final model and deployment                   |
  | Report nested CV performance    | For honest estimate of generalisation error      |

  </div>
</div>

:::

# More Advanced Search Spaces {#sec-advanced-search-space}

In this section, we look at the following:

- Using `to_tune()` to tune different scalar parameter classes;
- Using `ParamSet` to define your own search space (e.g. to handle tuning over vectors, transformations or handling parameter dependencies);
- Accessing a database of standardised search spaces from the literature.

## Scalar parameter tuning

The `to_tune()` function can be used to tune parameters of any class.

```{r}
learner = lrn("classif.svm",
  cost  = to_tune(1e-1, 1e5),
  kernel = to_tune(c("radial", "linear")), #<1>
  shrinking = to_tune(), #<2>
  type = "C-classification"
)

learner$param_set$search_space()
```
1. A factor, so pass the vector of levels required.
2. A logical, which `to_tune()` recognises automatically.

Numerical parameters can also be treated as factors if we want to discretise them over a small subset of possible values. For example;
```r
lrn("classif.ranger", num.trees = to_tune(c(100, 200, 400)))
```

## Defining Search Spaces with `ps`

The helper function `to_tune()` creates a parameter set that is then used by `tune()`, `ti()` or `auto_tuner()`.
However, sometimes you will need to create a parameter set manually using `ps()`.
This function takes names arguments of class `Domain`, which can be create using the sugar functions in the table below.

| Constructor | Description                            | Underlying Class |
|-------------|----------------------------------------|------------------|
| `p_dbl`     | Real valued parameter (“double”)       | `ParamDbl`       |
| `p_int`     | Integer parameter                      | `ParamInt`       |
| `p_fct`     | Discrete valued parameter (“factor”)   | `ParamFct`       |
| `p_lgl`     | Logical / Boolean parameter            | `ParamLgl`       |
| `p_uty`     | Untyped parameter                      | `ParamUty`       |

:`Domain` constructors and their resulting `Domain`.

A simple example: create a search space to tune `cost` `kernel` and `shrinking`,

```{r}
search_space = ps(
                  cost = p_dbl(lower = 1e-1, upper = 1e5),
                  kernel = p_fct(c("radial", "linear")),
                  shrinking = p_lgl()
                  )
```
We then pass this search space into `ti()`.

```{r}
ti(
   tsk_sonar, 
   lrn("classif.svm", type = "C-classification"),
   rsmp("cv", folds = 3),
   msr_ce,
   trm("none"),
   search_space = search_space
   )
```

For creating more advanced search spaces, see [Section
4.4.3](https://mlr3book.mlr-org.com/chapters/chapter4/hyperparameter_optimization.html#sec-tune-trafo)
(*Transformations and Tuning Over Vectors*)^[This section considers different
transformations of parameters, and scenarios where hyperparameters are
interdependent or need to be transformed jointly. For instance, when tuning
a support vector machine (SVM), the cost and gamma parameters often require
simultaneous optimization due to their combined effect on model performance.],
and [Section
4.4.5](https://mlr3book.mlr-org.com/chapters/chapter4/hyperparameter_optimization.html#sec-tuning-spaces)
for using `{mlr3tuningspaces}` to providing implementations of published search
spaces for many popular machine learning
algorithms.

# Conclusion

| Class                                                 | Constructor/Function               | Fields/Methods                          |
|--------------------------------------------------------|------------------------------------|------------------------------------------|
| `Terminator`                                           | `trm()`                            | –                                        |
| `TuningInstanceBatchSingleCrit` or `TuningInstanceBatchMultiCrit` | `ti()` / `tune()`             | `$result`; `$archive`                   |
| `TunerBatch`                                           | `tnr()`                            | `$optimize()`                           |
| `TuneToken`                                            | `to_tune()`                        | –                                        |
| `AutoTuner`                                            | `auto_tuner()`                     | `$train()`; `$predict()`; `$tuning_instance` |
| –                                                      | `extract_inner_tuning_results()`  | –                                        |
| –                                                      | `extract_inner_tuning_archives()` | –                                        |
| `ParamSet`                                             | `ps()`                             | –                                        |
| `TuningSpace`                                          | `lts()`                            | `$values`                                |

:Important classes and functions covered in this chapter with underlying class (if applicable), class constructor or function, and important class fields and methods (if applicable).

# Exercises

1. Tune the `mtry`, `sample.fraction`, and `num.trees` hyperparameters of `lrn("regr.ranger")` on `tsk("mtcars")`. Use a simple random search with 50 evaluations. Evaluate with a 3-fold CV and the root mean squared error. Visualize the effects that each hyperparameter has on the performance via simple marginal plots, which plot a single hyperparameter versus the cross-validated MSE.
2. Evaluate the performance of the model created in Exercise 1 with nested resampling. Use a holdout validation for the inner resampling and a 3-fold CV for the outer resampling.
3. Tune and benchmark an XGBoost model against a logistic regression (without tuning the latter) and determine which has the best Brier score. Use `mlr3tuningspaces` and nested resampling, try to pick appropriate inner and outer resampling strategies that balance computational efficiency vs. stability of the results.
4. (\*) Write a function that implements an iterated random search procedure that drills down on the optimal configuration by applying random search to iteratively smaller search spaces. Your function should have seven inputs: `task`, `learner`, `search_space`, `resampling`, `measure`, `random_search_stages`, and `random_search_size`. You should only worry about programming this for fully numeric and bounded search spaces that have no dependencies. In pseudo-code:

        a. Create a random design of size `random_search_size` from the given search space and evaluate the learner on it.
        b. Identify the best configuration.
        c. Create a smaller search space around this best config, where you define the new range for each parameter as: `new_range[i] = (best_conf[i] - 0.25 * current_range[i], best_conf[i] + 0.25*current_range[i])`. Ensure that this `new_range` respects the initial bound of the original `search_space` by taking the `max()` of the new and old lower bound, and the `min()` of the new and the old upper bound (“clipping”).
        d. Iterate the previous steps `random_search_stages` times and at the end return the best configuration you have ever evaluated. As a stretch goal, look into `mlr3tuning`’s internal source code and turn your function into an R6 class inheriting from the `TunerBatch` class – test it out on a learner of your choice.

## Question 1

Tune the `mtry`, `sample.fraction`, and `num.trees` hyperparameters of
`lrn("regr.ranger")` on `tsk("mtcars")`. Use a simple random search with 50
evaluations. Evaluate with a 3-fold CV and the root mean squared error.
Visualize the effects that each hyperparameter has on the performance via
simple marginal plots, which plot a single hyperparameter versus the
cross-validated MSE.

### Answer

Let's load the task and look at the properties of the `mtry`,
`sample.fraction`, and `num.trees` parameters.

```{r}
tsk_mtcars <- tsk("mtcars")
lrn("regr.ranger")$param_set$data[id %in% c("mtry", "sample.fraction", "num.trees")]
```

The hyperparameters I'm looking at are:

- `mtry`: number of variables considered at each tree split.
- `num.trees`: number of trees in the forest.
- `sample.fraction`: fraction of observations used to train each tree.

Now I will set up the tuning of the `mtry`, `sample.fraction`, and `num.trees` hyperparameters.

```{r}
learner <- lrn("regr.ranger",
               mtry = to_tune(p_int(1, 10)),
               num.trees = to_tune(20, 2000),
               sample.fraction = to_tune(0.1, 1))
learner
```

Setting up an instance to terminate the tuner after 50 evaluations, and to use 3-fold CV.

```{r}
instance <- ti(task = tsk_mtcars,
               learner = learner,
               resampling = rsmp("cv", folds = 3),
               # rmse gives interpretability in the original units (MPG) rather than squared units
               measures = msr("regr.rmse"), 
               terminator = trm("evals", n_evals = 50))
instance
```

The tuning step uses 3-fold cross-validation:

- In each evaluation, two-thirds of the data is used for training,
- One-third is used for validation (i.e. to compute the RMSE).
- This is repeated for 50 random configurations (as specified by the terminator).


Now I can set up the tuning process (random search).

```{r}
tuner <- tnr("random_search")
#tuner$param_set
tuner
```

And trigger the tuning process.

```{r}
set.seed(333)
tuner$optimize(instance)
instance$result
instance$result$learner_param_vals
```

All 50 of the random search evaluations are stored in the `archive` slot of the
`instance` object.
```{r}
as.data.table(instance$archive)[, .(mtry, sample.fraction, num.trees, regr.rmse)]
```

Now let's visualise the effect of each hyperparameter using marginal plots.

```{r}
#| fig-width: 8

autoplot(instance, type = "marginal", cols_x = c("mtry", "sample.fraction", "num.trees"))
```

As during the HPO stage, I used 3-fold CV, the model has not seen the full
data all at once. So, now I'll train the model using the optimised
hyperparameters.

```{r}
lrn_ranger_tuned <- lrn("regr.ranger")
lrn_ranger_tuned$param_set$values = instance$result_learner_param_vals
lrn_ranger_tuned$train(tsk_mtcars)$model
```

:::{.callout-note collapse="true"}
# Summary of question 1 (hyperparameter tuning with random search)

I tuned the `regr.ranger` learner on the `mtcars` dataset, focusing on three hyperparameters:

- `mtry`: number of variables considered at each split,
- `num.trees`: number of trees in the forest,
- `sample.fraction`: fraction of the dataset used for each tree.

**Steps I took:**

1. **Exploration**
   I inspected the available parameters for the learner using `$param_set`.

2. **Learner setup**
   I defined the tuning ranges with `to_tune()`:
   - `mtry` from 1 to 10,
   - `num.trees` from 1 to 100,000,
   - `sample.fraction` from 0.1 to 1.

3. **Tuning instance**
   I created a `TuningInstanceSingleCrit` using:
   - the `mtcars` task,
   - 3-fold cross-validation for resampling,
   - root mean squared error (RMSE) as the evaluation metric,
   - a limit of 50 evaluations using a random search strategy.

4. **Running the tuner**
   I used `tnr("random_search")` and called `$optimize()` to run the search. I then extracted the best hyperparameter combination and the archive of evaluated configurations.

5. **Visualising results**
   I used marginal plots to visualise the effect of each hyperparameter on the cross-validated RMSE.

6. **Training the final model**
   I retrained the `regr.ranger` model on the full dataset using the best parameters found.

:::

## Question 2

Evaluate the performance of the model created in Exercise 1 with nested
resampling. Use a holdout validation for the inner resampling and a 3-fold
CV for the outer resampling.

### Answer

OK, so here we need an outer and inner resampling strategy.
The outer resampling strategy will be a 3-fold CV, and the inner resampling
strategy will be a holdout validation.

![An illustration of nested resampling. The large blocks represent 3-fold CV for the outer resampling for model evaluation and the small blocks represent 4-fold CV for the inner resampling for HPO. The light blue blocks are the training sets and the dark blue blocks are the test sets.](./fig/nested_resampling.svg){#fig-nested-resampling-example}

@fig-nested-resampling-example shows an example of a nest resampling strategy
(with 3-fold CV on the outer and 4-fold CV on the inner nest). Here, we
need to do something slightly different as we are using the holdout resampling
strategy on the inner nest.

1. Outer resampling start
   - Perform 3-fold cross-validation on the full dataset.
   - For each outer fold, split the data into:
     - Training set (light blue blocks)
     - Test set (dark blue block)

2. Inner resampling
   - Within each outer training set, perform holdout validation (assuming 70/30 training-test split).
   - This inner split is used for tuning hyperparameters (not evaluation).

3. HPO – Hyperparameter tuning
   - Evaluate different hyperparameter combinations by fitting models on the inner training set and evaluating performance on the inner validation (holdout) set.
   - Select the best hyperparameter configuration based on performance on the inner holdout set. 

4. Training
   - Fit the model on the entire outer training set using the tuned hyperparameters.

5. Evaluation
   - Evaluate the trained model on the outer test set (unseen during tuning).

6. Outer resampling repeats
   - Repeat steps 2–5 for each of the 3 outer folds.

7. Aggregation
   - Average the 3 outer test performance scores.
   - This gives an unbiased estimate of the model’s generalisation performance with tuning.

I will use `AutoTuner` to do nested resampling, as that is what is done in
[Section
4.3.1](https://mlr3book.mlr-org.com/chapters/chapter4/hyperparameter_optimization.html#sec-nested-resampling)
of the mlr3 tutorial.

```{r}
## create auto_tuner to resample a random forest 
# with 3-fold CV in outer resampling and 
# holdout validation in inner resampling
at <- auto_tuner(
         tuner = tnr("random_search"), #<1>
         learner = lrn("regr.ranger",
                       mtry = to_tune(1, 1e1),
                       num.trees = to_tune(1, 1e5),
                       sample.fraction = to_tune(0.1, 1)),
         # inner resampling
         resampling = rsmp("holdout", ratio = 0.7),
         measure = msr("regr.rmse"),
         terminator = trm("evals", n_evals = 50)
        )

# resampling step
rr <- resample(tsk("mtcars"),
               at, 
               # outer resampling
               rsmp("cv", folds = 3), 
               store_models = TRUE) #<2>

rr
```
1. The tuners and learners are the same as in the previous exercise, I'm just
   defining them again here for clarity.
2. Set `store_models = TRUE` so that the `AutoTuner` models (fitted on the
   outer training data) are stored,

Now I aggregate across the three outer folds to get the final performance.
```{r}
rr$aggregate()
```

The inner tuning results can also be accessed, returning the optimal configurations (across all outer folds) and the full tuning archives.

```{r}
# optimal configurations
extract_inner_tuning_results(rr)
# full tuning archives
extract_inner_tuning_archives(rr)
```

:::{.callout-note collapse="true"}
# Summary of question 2 (nested resampling)

I evaluated the performance of the tuned model using nested resampling to obtain an unbiased estimate.

**Steps I took:**

1. **Resampling strategy**
   I used:
   - outer resampling: 3-fold cross-validation,
   - inner resampling: holdout validation with a 70/30 split.

2. **AutoTuner setup**
   I reused the same `regr.ranger` learner and parameter ranges as in Question 1, wrapped in an `AutoTuner`. The tuning again used 50 evaluations of random search and MSE as the measure.

3. **Resample execution**
   I called `resample()` with the outer CV and the `AutoTuner`, setting `store_models = TRUE` to keep the fitted models from each outer fold.

4. **Aggregating performance**
   I used `$aggregate()` to average the MSE across the outer test folds.

5. **Inspecting inner results**
   I extracted the best configurations and full tuning logs from each inner loop using `extract_inner_tuning_results()` and `extract_inner_tuning_archives()`.

:::

## Question 3

Tune and benchmark an XGBoost model against a logistic regression (without tuning the latter) and determine which has the best Brier score. Use `mlr3tuningspaces` and nested resampling, try to pick appropriate inner and outer resampling strategies that balance computational efficiency vs. stability of the results.

### Answer

I’ll use the built-in `spam` task -- since the outcome is categorical, this is a classification task.

```{r}
tsk_spam <- tsk("spam")
```

First I'll set up the logistic regression model (with no tuning).

```{r}
# requires probs to compute the brier score
lrn_logreg <- lrn("classif.log_reg", predict_type = "prob")
```

:::{.column-margin}

The XGBoost model has lots of hyperparameters:
```{r}
lrn("classif.xgboost")$param_set$ids()
```

The main ones are:

| Hyperparameter         | Description                                               | Type     |
|------------------------|-----------------------------------------------------------|----------|
| `eta`                  | Learning rate (shrinkage)                                 | numeric  |
| `max_depth`            | Maximum depth of trees                                    | integer  |
| `nrounds`              | Number of boosting rounds (trees)                         | integer  |
| `colsample_bytree`     | Fraction of features randomly sampled per tree            | numeric  |
| `subsample`            | Fraction of rows sampled per tree                         | numeric  |
| `min_child_weight`     | Minimum sum of instance weights in a child node           | numeric  |
| `gamma`                | Minimum loss reduction to make a split                    | numeric  |
| `lambda`               | L2 regularisation term on weights                         | numeric  |
| `alpha`                | L1 regularisation term on weights                         | numeric  |

A typical tuning strategy for XGBoost might involve:

1. Starting with basic tree shape and learning rate:
   - `max_depth`
   - `eta`
   - `nrounds`

2. Adding sampling and regularisation to control overfitting:
   - `subsample`
   - `colsample_bytree`
   - `min_child_weight`
   - `gamma`

3. Fine-tuning regularisation terms if needed:
   - `lambda` (L2)
   - `alpha` (L1)

:::

For the XGBoost learner, I'm going to use a predefined search space from
`{mlr3tuningspaces}`. First, I'll give a list of these predefined spaces.

```{r}
mlr_tuning_spaces$keys()[grepl("xgboost", mlr_tuning_spaces$keys())]
```

I will use the `classif.xgboost.default` space.
```{r}
space = lts("classif.xgboost.default")
space
```

Plugging this into `auto_tuner()` creates an `AutoTuner` object. I'm going to use
5-fold CV in the inner resampling and a terminator based on run time (of 60
seconds).

```{r}
# create terminator with time budget of 5 secs
trm_rt = trm("run_time")
trm_rt$param_set$values$secs = 5

# create xgboost learner with prob predict_type
# 'prob' required for brier score
lrn_xgb = lrn("classif.xgboost", predict_type = "prob")

at_xgb <- auto_tuner(learner = lrn_xgb,
                    resampling = rsmp("cv", folds = 5),
                    measure = msr("classif.bbrier"),
                    terminator = trm_rt,
                    tuner = tnr("random_search"),
                    search_space = space)
at_xgb
```

Now I can set up the outer resampling strategy (4-fold CV).

```{r}
outer_rsmp <- rsmp("cv", folds = 4)
```

I can create a benchmark grid and run it for the task to compare the two learners.

```{r}
# Benchmark both learners
design = benchmark_grid(
  tasks = tsk_spam,
  learners = list(lrn_logreg, at_xgb),
  resamplings = outer_rsmp
)
design

# run the benchmark design
set.seed(101)
bmr = benchmark(design)
# the score for each of the 4-fold CV outer folds
bmr$score(msr("classif.bbrier"))[, 
         .(learner_id, resampling_id, iteration, classif.bbrier)
         ]
# the aggregate score for each learner
bmr$aggregate(msr("classif.bbrier"))[,
         .(learner_id, resampling_id, classif.bbrier)
         ]
```

I can use `autoplot` to plot these results.
```{r}
autoplot(bmr, measure = msr("classif.bbrier"))
```

So, XGBoost performs better than the logistic regression model on the `spam`
task. But, the XGBoost model is much more computationally expensive, takes
longer to train, and is less interpretable. So, the choice of model is a trade
off between performance and interpretability.

:::{.callout-note collapse="true"}
# Summary of question 3 (XGBoost vs. logistic regression)

I benchmarked a tuned XGBoost model against an untuned logistic regression model on the `spam` classification task using the Brier score.

**Steps I took:**

1. **Loading the task**  
   I used the built-in `tsk("spam")`.

2. **Logistic regression setup**  
   I defined a `classif.log_reg` learner with `predict_type = "prob"` to enable Brier score calculation.

3. **XGBoost setup with tuning**  
   I used `classif.xgboost` with `predict_type = "prob"` and the predefined tuning space `lts("classif.xgboost.default")` from `{mlr3tuningspaces}`.

4. **AutoTuner for XGBoost**  
   I created an `AutoTuner` with:
   - 5-fold CV for inner resampling,  
   - 60-second time budget via `trm("run_time")`,  
   - random search tuner,  
   - Brier score as the measure.

5. **Outer resampling**  
   I used 4-fold CV for the outer loop.

6. **Benchmark setup and execution**  
   I created a benchmark grid comparing both learners on the task, ran the benchmark, and scored the results using the Brier score.

7. **Results**  
   I looked at individual fold scores using `bmr$score()` and aggregate performance using `bmr$aggregate()`. I also visualised the comparison with `autoplot()`.
:::

## Question 4
search

Write a function that implements an iterated random search procedure that drills down on the optimal configuration by applying random search to iteratively smaller search spaces. Your function should have seven inputs: `task`, `learner`, `search_space`, `resampling`, `measure`, `random_search_stages`, and `random_search_size`. You should only worry about programming this for fully numeric and bounded search spaces that have no dependencies. In pseudo-code:

    a. Create a random design of size `random_search_size` from the given search space and evaluate the learner on it.
    b. Identify the best configuration.
    c. Create a smaller search space around this best config, where you define the new range for each parameter as: `new_range[i] = (best_conf[i] - 0.25 * current_range[i], best_conf[i] + 0.25*current_range[i])`. Ensure that this `new_range` respects the initial bound of the original `search_space` by taking the `max()` of the new and old lower bound, and the `min()` of the new and the old upper bound (“clipping”).
    d. Iterate the previous steps `random_search_stages` times and at the end return the best configuration you have ever evaluated. As a stretch goal, look into `mlr3tuning`’s internal source code and turn your function into an R6 class inheriting from the `TunerBatch` class – test it out on a learner of your choice.

### Answer

#### Example on `mtcars` dataset

Let's start by trying this out on the `mtcars` dataset, using the `regr.rpart` learner. I'll make it simpler by using the `to_tune()` function, which means I don't have to define the search space manually to be *e.g.* an integer, double *etc.*.

First, I'll load the task and learner, and then look at the hyperparameters of this learner.

```{r}
#| cache: false
task <- tsk("mtcars")
learner <- lrn("regr.rpart")
# look at the hyperparameters
as.data.table(learner$param_set)[, .(id, class, lower, upper)]
```

:::{.column-margin}

| ID           | Description                                                             | Typical Range           | Notes                                    |
|--------------|-------------------------------------------------------------------------|--------------------------|-------------------------------------------|
| `cp`         | Complexity parameter: controls cost of adding splits                    | [0.001, 0.1] (log-scale) | Lower values allow deeper trees           |
| `maxdepth`   | Maximum depth of the tree                                               | 1–30                     | Prevents trees from growing too deep      |
| `minsplit`   | Minimum number of observations required to attempt a split              | 2–20                     | Higher values make trees more conservative |
| `minbucket`  | Minimum number of observations in any terminal node                     | 1–10                     | If not set, defaults to `minsplit / 3`    |

: Common hyperparameters to tune for `regr.rpart` learner.

:::

**Part 1:** creating a search_space and evaluating the learner

I'll now create a search space to tune the four hyperparameters `cp`,
`maxdepth`, `minsplit`, and `minbucket`, using `to_tune()`.

```{r}
#| cache: false

# use to_tune() to create the search space
learner$param_set$values$cp <- to_tune(1e-4, 0.1)
learner$param_set$values$maxdepth <- to_tune(1, 30)
learner$param_set$values$minsplit <- to_tune(2, 20)
learner$param_set$values$minbucket <- to_tune(1, 10)

param_ids <- names(Filter(function(x) inherits(x, "TuneToken"), learner$param_set$values))
# filter param_set bu param_names
learner$param_set$values[param_ids]
```

Now I'll tune the learner on this search space using a random search with 50 evaluations and 3-fold CV. 

```{r}
#| cache: false

# create random search tuner
tuner <- tnr("random_search")

# create tuning instance
instance <- mlr3tuning::tune( # <1>
  tuner = tuner,
  task = task,
  learner = learner,
  resampling = rsmp("cv", folds = 3),
  measure = msr("regr.rmse"),
  terminator = trm("evals", n_evals = 50)
)


# run the tuning
set.seed(333)
tuner$optimize(instance)
```
1. I need to remember to call the correct `tune()` - the one from `{mlr3tuning}` instead of from `{e1701}`. If the latter is used, you get an error like `argument "train.x" is missing, with no default`.

**Part 2:** Identifying the best configuration

```{r}
# get the best configuration
best_config <- instance$result_learner_param_vals[param_ids]
unlist(best_config)
```

**Part 3:** create a smaller search space around the best configuration

```{r}
# obtain new upper and lower bounds
# helper function to adjust bounds

adjust_bounds <- function(param_id, best_config, learner, shrink = 0.25) {
  param = learner$param_set$values[[param_id]]$content
  bounds = list(lower = param$lower, upper = param$upper)
  best = best_config[[param_id]]
  range = bounds$upper - bounds$lower
  lower_bound <- best - shrink * range
  upper_bound <- best + shrink * range
  # if class is ParamInt, then set lower to be ceiling and upper to be floor
  # obtain class of the parameter
  param_class <- learner$param_set$params[id == param_id, setNames(cls, id)]
  if (param_class == "ParamInt") {
    lower_bound = ceiling(lower_bound)
    upper_bound = floor(upper_bound)
  }
  lower_new = max(lower_bound, bounds$lower)
  upper_new = min(upper_bound, bounds$upper)
  list(lower = lower_new, upper = upper_new)
}

adjusted_bounds <- lapply(param_ids, 
                          adjust_bounds,
                          best_config = best_config,
                          learner = learner)

names(adjusted_bounds) <- param_ids

# update learner
learner$param_set$values$cp <- to_tune(adjusted_bounds$cp$lower, 
                                       adjusted_bounds$cp$upper)
learner$param_set$values$maxdepth <- to_tune(adjusted_bounds$maxdepth$lower,
                                             adjusted_bounds$maxdepth$upper)
learner$param_set$values$minsplit <- to_tune(adjusted_bounds$minsplit$lower,
                                             adjusted_bounds$minsplit$upper)
learner$param_set$values$minbucket <- to_tune(adjusted_bounds$minbucket$lower,
                                              adjusted_bounds$minbucket$upper)
Filter(function(x) inherits(x, "TuneToken"), learner$param_set$values)

```

**Part 4:** evaluate the learner on the new search space

```{r}
# create new tuning instance
set.seed(333)
instance <- mlr3tuning::tune(
  tuner = tuner,
  task = task,
  learner = learner,
  resampling = rsmp("cv", folds = 3),
  measure = msr("regr.rmse"),
  terminator = trm("evals", n_evals = 50),
)

tuner$optimize(instance)
```

Get the best configuration from this:

```{r}
# get the best configuration
best_config <- instance$result_learner_param_vals[param_ids]
unlist(best_config)
```

Cool, so I have a new best configuration. I can repeat this process iteratively
on smaller subspaces.


# Summary

Great. Let's summarise what I've done in this post.

**Exercise 1: Hyperparameter Tuning with Random Search**

- Tunes `regr.ranger` on `mtcars` dataset
- Parameters tuned: `mtry`, `num.trees`, `sample.fraction`
- Uses 3-fold CV, 50 random evaluations, MSE as the measure
- Visualises marginal effects of hyperparameters
- Retrains final model on full data using best hyperparameters

**Exercise 2: Nested Resampling**

- Evaluates tuned model’s performance with nested resampling
- Outer loop: 3-fold CV; Inner loop: holdout validation (70/30)
- Uses `AutoTuner` with same hyperparameter setup as Q1
- Aggregates performance across outer test folds
- Extracts inner tuning results and archives

**Exercise 3: Benchmarking XGBoost vs Logistic Regression**

- Task: binary classification on `spam` dataset
- Logistic regression used as untuned baseline
- XGBoost tuned using `mlr3tuningspaces::lts("classif.xgboost.default")`
- Inner loop: 5-fold CV with 60 sec time budget; Outer: 4-fold CV
- Evaluates models using Brier score
- Compares learners via tables and visualisation

# Fin

